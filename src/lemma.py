# -*- coding: utf-8 -*-
"""Lemma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LFxX33gnO3nI8_sRprgNAQHfSxI2OS1i
"""

import pandas as pd

# Sample list; later replace with full 600-word dataset
twi_words = [
    'kɔ', 'kɔɔ', 'kɔeɛ', 'bɛkɔ', 'rekɔ', 'meyɛ', 'yɛɛ', 'medan', 'ɔmofie',
    'bɛyɛ', 'ɔyɛ', 'di', 'didii', 'redidi', 'bedi', 'mebisaeɛ', 'rekyerɛ',
    'kasa', 'ka', 'bɛka', 'rekasa', 'ɛkɔm', 'adidi', 'adwane', 'asikafoɔ',
    'mmoa', 'abɔfra', 'rekai', 'resa', 'tueɛ', 'ɔfaeɛ', 'nnua', 'osikanii',
    'sɔre3', 'nhunumuu', 'hwɛsofoc', 'anuanom', 'ɔdɔ', 'ɔbarima', 'mmrante3',
    'ɛkwan', 'Afuo', 'nwura', 'kutaeɛ', 'b3su', 'nsɛmbisa', 'nkwerɛeɛ', 'repia',
    'bɛba', 'nkasa','aba'





]

# Convert to DataFrame
df = pd.DataFrame(twi_words, columns=['Word'])

# Prefixes and suffixes (simplified rule base)
prefixes = ['re', 'bɛ', 'me', 'mɛ', 'ɔmo', 'be', 'ɛ', 'o', 'wo', 'n','be', 'a', 'mo', 'ɔ', 'sɛm' ]
suffixes = [ 'foɔ', 'dii', 'di', 'nii', 'eɛ', 'so', 'nom', 'kuo', 'ɛ', 'ɔ', 'muu', ]


# Lemma dictionary (simplified for now)
lemma_dict = {
    'kɔ': 'kɔ',
    'yɛ': 'yɛ',
    'di': 'di',
    'ka': 'ka',
    'kasa': 'kasa',
    'pɛ': 'pɛ',
    'fwe': 'fwe'
}


def lemmatize(word):
    rule_applied = []
    original_word = word # Keep the original word for rule description

    # Example transformation rule: Change 're' prefix to 'ko' (This is a placeholder, replace with actual rules)
    if word.startswith('nn') and len(word) > len('nn') + 1:
        word = 'd' + word[len('nn'):]
        rule_applied.append(f'Prefix transformation: nn -> d')

    if word.startswith('mm') and len(word) > len('mm') + 1:
        word = 'ab' + word[len('mm'):]
        rule_applied.append(f'Prefix transformation: mm -> ad')

    # Remove prefix
    for pre in prefixes:
        if word.startswith(pre) and len(word) > len(pre) + 1:
            word = word[len(pre):]
            rule_applied.append(f'Prefix removal: {pre}')
            break  # Apply one rule at a time

    # Remove suffix
    for suf in suffixes:
        if word.endswith(suf) and len(word) > len(suf) + 1:
            word = word[:-len(suf)]
            rule_applied.append(f'Suffix removal: {suf}')
            break


    return word, ", ".join(rule_applied) if rule_applied else "None"





    # Apply lemmatizing
df['Lemma'], df['lemma Rule Applied'] = zip(*df['Word'].apply(lemmatize))

# Show the result after lemmatizing
print("After Lemmatizing:")
display(df[['Word', 'lemma Rule Applied', 'Lemma']])

def stem(word):
    rule_applied = []
    original_word = word # Keep the original word for rule description


    # Remove prefix
    for pre in prefixes:
        if word.startswith(pre) and len(word) > len(pre) + 1:
            word = word[len(pre):]
            rule_applied.append(f'Prefix removal: {pre}')
            break  # Apply one rule at a time

    # Remove suffix
    for suf in suffixes:
        if word.endswith(suf) and len(word) > len(suf) + 1:
            word = word[:-len(suf)]
            rule_applied.append(f'Suffix removal: {suf}')
            break


    return word, ", ".join(rule_applied) if rule_applied else "None"


    # Apply stemming
df['Stem'], df['stem Rule Applied'] = zip(*df['Word'].apply(stem))

# Show the result after stemming
print("After Stemming:")
display(df[['Word', 'stem Rule Applied', 'Stem']])

user_word = input("Enter a word: ")
choice = input("Enter 'lemma' for lemmatization or 'stem' for stemming: ").lower()

if choice == 'lemma':
    result, rule = lemmatize(user_word)
    print(f"\nOriginal Word: {user_word}")
    print(f"Rule Applied: {rule}")
    print(f"Lemma: {result}")
elif choice == 'stem':
    result, rule = stem(user_word)
    print(f"\nOriginal Word: {user_word}")
    print(f"Rule Applied: {rule}")
    print(f"Stem: {result}")
else:
    print("Invalid choice. Please enter 'lemma' or 'stem'.")